#!/bin/bash
#PBS -q gpu
#PBS -l walltime=48:00:00  ## maximum runtime of the job is 72 hours
#PBS -l pvmem=32gb         ## memory per process, should be at least 32g when using GPUs
#PBS -l nodes=1:ppn=28:gpus=2 ## maximum 2 GPU per node, each GPU is controlled by one process, ppn is the processors (cores) per node
#PBS -N qsub_job              ## name of the job, can be set to whatever you like
#PBS -m bea # begin end abort send email notification
#PBS -M <yz681@leicester.ac.uk>
#PBS -o qsub.log   ## output file for normal log
#PBS -e qsub.log   ## output file for errors



#module load cuda90/cudnn/7.0
#module load cuda90/toolkit/9.0.176
#module load python/gcc/3.9.10
#python --version
source /lustre/ahome3/y/yz681/py36/bin/activate
python --version

cd /lustre/ahome3/y/yz681/ic/
# https://github.com/NVlabs/stylegan2-ada-pytorch/issues/190
#pip install torch==1.7.1+cu110 torchvision==0.8.2+cu110 torchaudio===0.7.2 -f https://download.pytorch.org/whl/torch_stable.html
#pip install -r requirements.txt
ip1=`hostname -I | awk '{print $1}'`

echo $ip1

python train.py --cfg config/iNaturalist2018/Res50_iNaturalist2018_0.yaml --cfg_specify config/specify/batch64.yaml --nodes 2 --gpus 2 --nr 0 --ip $ip1 --port 40706 --wks 10
#python train.py --cfg config/cifar/Res32_cifar10_imbalance_0-02.yaml --nodes 1 --gpus 2 --nr 0 --ip $ip1 --port 47564 --wks 26
#python train.py --cfg config/mnist/Res32_mnist.yaml --nodes 1 --gpus 2 --nr 0 --ip $ip1 --port 47564 --wks 26

